{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解决此类问题的一般步骤\n",
    "1. 收集数据：提供文本文件\n",
    "2. 准备数据：将文本文件解析成词条向量。\n",
    "3. 分析数据：检查词条确保解析的正确性。\n",
    "4. 训练算法：使用下面的trainNB()函数\n",
    "5. 测试算法：使用classify()，并且构建一个新的测试函数来计算文档集的错误率。\n",
    "6. 使用算法：构建一个完整的程序对一组文档进行分类，将错分的文档输出到屏幕上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category                                               text\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "5     spam  FreeMsg Hey there darling it's been 3 week's n..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw_data = pd.read_csv('./asset/data.txt', sep='\\t')\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ham', 'ham', 'spam', 'ham', 'ham', 'spam']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ListClasses是对应的训练内容的分类\n",
    "ListClasses = []\n",
    "for index in range(len(raw_data)):\n",
    "    ListClasses.append(raw_data.iloc[index].category)\n",
    "\n",
    "ListClasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 0, 1]\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "def tokenize(sms):\n",
    "    return sms.split(' ')\n",
    "\n",
    "# ListPost是训练内容\n",
    "ListPost = []\n",
    "# ListClasses是对应的训练内容的分类\n",
    "ListClasses = []\n",
    "for content in range(len(raw_data)):\n",
    "    ListPost.append(tokenize(raw_data.iloc[content].text))\n",
    "    # spam是1，ham是0\n",
    "    if raw_data.iloc[content].category == 'ham':\n",
    "        ListClasses.append(0)\n",
    "    else:\n",
    "        ListClasses.append(1)\n",
    "    \n",
    "print(ListClasses)\n",
    "\n",
    "\n",
    "# 创建一个包含所有文档中出现的不重复单词的列表, dataset为训练文本\n",
    "def createVocabList(dataset):\n",
    "    vocabSet = set([])\n",
    "    for document in dataset:\n",
    "#         print(document)\n",
    "        vocabSet = vocabSet | set(document) # 两个set取并集\n",
    "#         print(vocabSet)\n",
    "    return list(vocabSet)\n",
    "myVocabList = createVocabList(ListPost)\n",
    "print(len(myVocabList))\n",
    "\n",
    "# 词袋模型，每个词可以出现多次\n",
    "def bagOfWords2VecMN(vocabList, inputSet):\n",
    "    Vector = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            Vector[vocabList.index(word)]+=1\n",
    "    return Vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.006711409395973154,\n",
       "  0.006711409395973154,\n",
       "  0.006711409395973154,\n",
       "  0.013422818791946308,\n",
       "  0.006711409395973154,\n",
       "  0.006711409395973154,\n",
       "  0.013422818791946308,\n",
       "  0.006711409395973154,\n",
       "  0.020134228187919462,\n",
       "  0.006711409395973154,\n",
       "  0.013422818791946308,\n",
       "  0.013422818791946308,\n",
       "  0.013422818791946308,\n",
       "  0.006711409395973154,\n",
       "  0.006711409395973154,\n",
       "  0.006711409395973154,\n",
       "  0.006711409395973154,\n",
       "  0.013422818791946308,\n",
       "  0.006711409395973154,\n",
       "  0.013422818791946308,\n",
       "  0.013422818791946308,\n",
       "  0.013422818791946308,\n",
       "  0.006711409395973154,\n",
       "  0.013422818791946308,\n",
       "  0.013422818791946308,\n",
       "  0.013422818791946308,\n",
       "  0.013422818791946308,\n",
       "  0.013422818791946308,\n",
       "  0.006711409395973154,\n",
       "  0.006711409395973154,\n",
       "  0.006711409395973154,\n",
       "  0.006711409395973154,\n",
       "  0.013422818791946308,\n",
       "  0.006711409395973154,\n",
       "  0.006711409395973154,\n",
       "  0.006711409395973154,\n",
       "  0.013422818791946308,\n",
       "  0.013422818791946308,\n",
       "  0.013422818791946308,\n",
       "  0.006711409395973154,\n",
       "  0.006711409395973154,\n",
       "  0.006711409395973154,\n",
       "  0.006711409395973154,\n",
       "  0.013422818791946308,\n",
       "  0.006711409395973154,\n",
       "  0.013422818791946308,\n",
       "  0.006711409395973154,\n",
       "  0.013422818791946308,\n",
       "  0.013422818791946308,\n",
       "  0.006711409395973154,\n",
       "  0.013422818791946308,\n",
       "  0.013422818791946308,\n",
       "  0.013422818791946308,\n",
       "  0.006711409395973154,\n",
       "  0.006711409395973154,\n",
       "  0.013422818791946308,\n",
       "  0.013422818791946308,\n",
       "  0.006711409395973154,\n",
       "  0.013422818791946308,\n",
       "  0.006711409395973154,\n",
       "  0.006711409395973154,\n",
       "  0.020134228187919462,\n",
       "  0.006711409395973154,\n",
       "  0.006711409395973154,\n",
       "  0.006711409395973154,\n",
       "  0.013422818791946308,\n",
       "  0.006711409395973154,\n",
       "  0.006711409395973154,\n",
       "  0.013422818791946308,\n",
       "  0.013422818791946308,\n",
       "  0.006711409395973154,\n",
       "  0.006711409395973154,\n",
       "  0.013422818791946308,\n",
       "  0.013422818791946308,\n",
       "  0.013422818791946308,\n",
       "  0.006711409395973154,\n",
       "  0.013422818791946308,\n",
       "  0.006711409395973154,\n",
       "  0.013422818791946308,\n",
       "  0.006711409395973154,\n",
       "  0.013422818791946308,\n",
       "  0.013422818791946308,\n",
       "  0.006711409395973154,\n",
       "  0.013422818791946308,\n",
       "  0.013422818791946308,\n",
       "  0.013422818791946308,\n",
       "  0.013422818791946308,\n",
       "  0.013422818791946308,\n",
       "  0.006711409395973154,\n",
       "  0.013422818791946308,\n",
       "  0.013422818791946308,\n",
       "  0.006711409395973154,\n",
       "  0.006711409395973154,\n",
       "  0.006711409395973154,\n",
       "  0.006711409395973154,\n",
       "  0.006711409395973154,\n",
       "  0.006711409395973154,\n",
       "  0.013422818791946308,\n",
       "  0.006711409395973154],\n",
       " [0.012578616352201259,\n",
       "  0.012578616352201259,\n",
       "  0.012578616352201259,\n",
       "  0.006289308176100629,\n",
       "  0.012578616352201259,\n",
       "  0.012578616352201259,\n",
       "  0.006289308176100629,\n",
       "  0.018867924528301886,\n",
       "  0.006289308176100629,\n",
       "  0.012578616352201259,\n",
       "  0.006289308176100629,\n",
       "  0.006289308176100629,\n",
       "  0.006289308176100629,\n",
       "  0.012578616352201259,\n",
       "  0.012578616352201259,\n",
       "  0.012578616352201259,\n",
       "  0.012578616352201259,\n",
       "  0.006289308176100629,\n",
       "  0.012578616352201259,\n",
       "  0.006289308176100629,\n",
       "  0.006289308176100629,\n",
       "  0.006289308176100629,\n",
       "  0.012578616352201259,\n",
       "  0.006289308176100629,\n",
       "  0.006289308176100629,\n",
       "  0.006289308176100629,\n",
       "  0.006289308176100629,\n",
       "  0.006289308176100629,\n",
       "  0.012578616352201259,\n",
       "  0.012578616352201259,\n",
       "  0.012578616352201259,\n",
       "  0.012578616352201259,\n",
       "  0.006289308176100629,\n",
       "  0.012578616352201259,\n",
       "  0.012578616352201259,\n",
       "  0.012578616352201259,\n",
       "  0.006289308176100629,\n",
       "  0.006289308176100629,\n",
       "  0.006289308176100629,\n",
       "  0.012578616352201259,\n",
       "  0.012578616352201259,\n",
       "  0.012578616352201259,\n",
       "  0.012578616352201259,\n",
       "  0.006289308176100629,\n",
       "  0.012578616352201259,\n",
       "  0.006289308176100629,\n",
       "  0.012578616352201259,\n",
       "  0.006289308176100629,\n",
       "  0.006289308176100629,\n",
       "  0.012578616352201259,\n",
       "  0.006289308176100629,\n",
       "  0.006289308176100629,\n",
       "  0.006289308176100629,\n",
       "  0.012578616352201259,\n",
       "  0.012578616352201259,\n",
       "  0.006289308176100629,\n",
       "  0.012578616352201259,\n",
       "  0.012578616352201259,\n",
       "  0.006289308176100629,\n",
       "  0.012578616352201259,\n",
       "  0.012578616352201259,\n",
       "  0.006289308176100629,\n",
       "  0.012578616352201259,\n",
       "  0.018867924528301886,\n",
       "  0.012578616352201259,\n",
       "  0.006289308176100629,\n",
       "  0.012578616352201259,\n",
       "  0.012578616352201259,\n",
       "  0.006289308176100629,\n",
       "  0.006289308176100629,\n",
       "  0.012578616352201259,\n",
       "  0.012578616352201259,\n",
       "  0.012578616352201259,\n",
       "  0.006289308176100629,\n",
       "  0.006289308176100629,\n",
       "  0.012578616352201259,\n",
       "  0.006289308176100629,\n",
       "  0.012578616352201259,\n",
       "  0.006289308176100629,\n",
       "  0.012578616352201259,\n",
       "  0.006289308176100629,\n",
       "  0.006289308176100629,\n",
       "  0.012578616352201259,\n",
       "  0.03773584905660377,\n",
       "  0.006289308176100629,\n",
       "  0.006289308176100629,\n",
       "  0.006289308176100629,\n",
       "  0.006289308176100629,\n",
       "  0.012578616352201259,\n",
       "  0.006289308176100629,\n",
       "  0.006289308176100629,\n",
       "  0.012578616352201259,\n",
       "  0.012578616352201259,\n",
       "  0.012578616352201259,\n",
       "  0.012578616352201259,\n",
       "  0.012578616352201259,\n",
       "  0.012578616352201259,\n",
       "  0.006289308176100629,\n",
       "  0.012578616352201259],\n",
       " 0.3333333333333333)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def trainNB(trainMatrix,trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)  # 总文档数\n",
    "    numWords = len(trainMatrix[0]) # Vocabulary中有多少个 words\n",
    "    # p(ci) = 类ci下的文档数 / 总文档数\n",
    "    pSpam = sum(trainCategory)/float(numTrainDocs)  # 这里是p(c1) 垃圾邮件数 / 总邮件数\n",
    "    p0Num = [0 for i in range(numWords)]  # 产生长度为 numWords 的全0 lis\n",
    "#     print(p0Num)\n",
    "    p1Num = [0 for i in range(numWords)]\n",
    "    p0Denom, p1Denom = 0.0,0.0\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            subMatrix = trainMatrix[i]\n",
    "            for a in range(numWords):\n",
    "                summ = p1Num[a]+subMatrix[a]\n",
    "                p1Num[a] = summ\n",
    "#             print(subMatrix)\n",
    "#             print(p1Num)\n",
    "            p1Denom += sum(trainMatrix[i])  # 类c1下，单词的总数\n",
    "        else:\n",
    "            subMatrix = trainMatrix[i]\n",
    "            for a in range(numWords):\n",
    "                summ = p0Num[a]+subMatrix[a]\n",
    "                p0Num[a] = summ\n",
    "#             p0Num += trainMatrix[i]  # 计数，\n",
    "#             print(subMatrix)\n",
    "#             print(p0Num)\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    p1Vector = []\n",
    "    p0Vector = []\n",
    "#     print(p0Num)\n",
    "#     print(p0Denom)\n",
    "    a = 1  # Laplace smoothing\n",
    "    for i in range(numWords):\n",
    "        p1 = (p1Num[i] + a) / (p1Denom + a*numWords)\n",
    "        p0 = (p0Num[i] + a) / (p0Denom + a*numWords)\n",
    "#         p1 = p1Num[i]/p1Denom\n",
    "#         p0 = p0Num[i]/p0Denom\n",
    "        p1Vector.append(p1)\n",
    "        p0Vector.append(p0)\n",
    "#     p1Vector = p1Num/p1Denom\n",
    "#     p0Vector = p0Num/p0Denom\n",
    "#     a = 1\n",
    "#     p1Vector = (p1Num + a) / (p1Denom + a*numWords)\n",
    "#     p0Vector = (p0Num + a) / (p0Denom + a*numWords)\n",
    "    return p0Vector,p1Vector,pSpam\n",
    "\n",
    "trainMat = []\n",
    "for eachDoc in ListPost:\n",
    "    trainMat.append(bagOfWords2VecMN(myVocabList,eachDoc))\n",
    "# len(trainMat)\n",
    "trainNB(trainMat,ListClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0V,p1V,pSpam = trainNB(trainMat,ListClasses)\n",
    "pSpam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(testEntry):\n",
    "    p0 = 1\n",
    "    p1 = 1\n",
    "    for i in range(len(testEntry)):\n",
    "        if testEntry[i] in myVocabList:\n",
    "    #         print(myVocabList.index(testEntry[i]))\n",
    "            p0i = p0V[myVocabList.index(testEntry[i])]\n",
    "            p0 = p0*p0i\n",
    "            p1i = p1V[myVocabList.index(testEntry[i])]\n",
    "            p1 = p1*p1i\n",
    "    p0 = p0*(1-pSpam)\n",
    "    p1 = p1*pSpam\n",
    "    # print(p0)\n",
    "    print(p1/p0)\n",
    "    return p1/p0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2342767295597484\n"
     ]
    }
   ],
   "source": [
    "sms = 'I am not spam'\n",
    "\n",
    "testEntry = tokenize(sms)\n",
    "a = classify(testEntry)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
